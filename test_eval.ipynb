{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import argparse\n",
    "from pytorch_lightning.utilities.types import EVAL_DATALOADERS\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import pytorch_lightning as pl\n",
    "from lightning.pytorch.accelerators import find_usable_cuda_devices\n",
    "from asteroid.data import DNSDataset,LibriMix\n",
    "from asteroid.models import DCCRNet, DCCRNet_mini\n",
    "from asteroid.losses import PITLossWrapper, pairwise_neg_sisdr\n",
    "from tools_for_model import near_avg_index, max_index, min_index, Bar\n",
    "import yaml\n",
    "from pprint import pprint\n",
    "from asteroid.utils import prepare_parser_from_dict, parse_args_as_dict\n",
    "from asteroid.metrics import get_metrics\n",
    "from dataloader import create_dataloader\n",
    "from tools_for_model import cal_pesq, cal_stoi\n",
    "from torch_stoi import NegSTOILoss\n",
    "from asteroid.utils import tensors_to_device\n",
    "from asteroid.dsp.normalization import normalize_estimates\n",
    "from framework import MultiResolutionSTFTLoss, SPKDLoss, build_review_kd\n",
    "import feature_extraction\n",
    "import config as cfg\n",
    "from asteroid.metrics import WERTracker, MockWERTracker\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop 0 utterances from 3000 (shorter than 3 seconds)\n"
     ]
    }
   ],
   "source": [
    "data_set = LibriMix(\n",
    "            csv_dir='/root/NTH_student/Speech_Enhancement_new/knowledge_distillation_CLSKD/data/wav16k/min/dev',\n",
    "            task='enh_single',\n",
    "            sample_rate=16000,\n",
    "            n_src=1,\n",
    "            segment=3,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloader(mode='valid',dataset=data_set)\n",
    "device = torch.device('cpu')\n",
    "\n",
    "COMPUTE_METRICS = [\"stoi\"]\n",
    "#COMPUTE_METRICS = [\"si_sdr\", \"sdr\", \"sir\", \"sar\", \"stoi\"]\n",
    "wer_tracker = (MockWERTracker())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  96/3000: [=>................................................] - ETA 4.7s"
     ]
    }
   ],
   "source": [
    "X,Y = 0,0\n",
    "i = 0\n",
    "for x,y in Bar(train_loader):\n",
    "    if i == 2: break\n",
    "    X=x\n",
    "    Y=y\n",
    "    i += 1\n",
    "#model = DCCRNet.from_pretrained('JorisCos/DCCRNet_Libri1Mix_enhsingle_16k')\n",
    "model = DCCRNet_mini.from_pretrained('/root/NTH_student/Speech_Enhancement_new/knowledge_distillation_CLSKD/checkpoint/the_best_model.pth')\n",
    "loss_func = PITLossWrapper(pairwise_neg_sisdr, pit_from=\"pw_mtx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_device = next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_save_ex = 5\n",
    "sample_rate = 16000\n",
    "ex_save_dir = '/root/NTH_student/Speech_Enhancement_new/knowledge_distillation_CLSKD/example_CLSKD'\n",
    "series_list = []\n",
    "torch.no_grad().__enter__()\n",
    "save_idx = random.sample(range(len(data_set)), n_save_ex)\n",
    "\n",
    "for idx in range(len(X)):\n",
    "    mix = X[idx]\n",
    "    sources = Y[idx]\n",
    "    mix, sources = tensors_to_device([mix, sources], device=model_device)\n",
    "    est_sources = model(mix.unsqueeze(0))\n",
    "    loss, reordered_sources = loss_func(est_sources, sources[None], return_est=True)\n",
    "    mix_np = mix.cpu().data.numpy()\n",
    "    sources_np = sources.cpu().data.numpy()\n",
    "    est_sources_np = reordered_sources.squeeze(0).cpu().data.numpy()\n",
    "    # For each utterance, we get a dictionary with the mixture path,\n",
    "    # the input and output metrics\n",
    "    utt_metrics = get_metrics(\n",
    "        mix_np,\n",
    "        sources_np,\n",
    "        est_sources_np,\n",
    "        sample_rate=16000,\n",
    "        metrics_list=COMPUTE_METRICS)\n",
    "    utt_metrics[\"mix_path\"] = data_set.mixture_path\n",
    "    est_sources_np_normalized = normalize_estimates(est_sources_np, mix_np)\n",
    "    utt_metrics.update(\n",
    "        **wer_tracker(\n",
    "            mix=mix_np,\n",
    "            clean=sources_np,\n",
    "            estimate=est_sources_np_normalized,\n",
    "            sample_rate=sample_rate,\n",
    "        )\n",
    "    )\n",
    "    series_list.append(pd.Series(utt_metrics))\n",
    "\n",
    "\n",
    "    #save some examples\n",
    "    if idx in save_idx:\n",
    "        local_save_dir = os.path.join(ex_save_dir, \"ex_{}/\".format(idx))\n",
    "        os.makedirs(local_save_dir, exist_ok=True)\n",
    "        sf.write(local_save_dir + \"mixture.wav\", mix_np, sample_rate)\n",
    "        # Loop over the sources and estimates\n",
    "        for src_idx, src in enumerate(sources_np):\n",
    "            sf.write(local_save_dir + \"s{}.wav\".format(src_idx), src, sample_rate)\n",
    "        for src_idx, est_src in enumerate(est_sources_np_normalized):\n",
    "            sf.write(\n",
    "                local_save_dir + \"s{}_estimate.wav\".format(src_idx),\n",
    "                est_src,\n",
    "                sample_rate,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('input_stoi', 0.7868099251140626), ('stoi', 0.872142777344521), ('mix_path', '/root/NTH_student/data/Libri2Mix/wav16k/min/dev/mix_single/2277-149874-0002_2412-153947-0014.wav')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utt_metrics.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall metrics :\n",
      "{'stoi': 0.8651579246456313, 'stoi_imp': 0.07627895924089204}\n"
     ]
    }
   ],
   "source": [
    "all_metrics_df = pd.DataFrame(series_list)\n",
    "\n",
    " # Print and save summary metrics\n",
    "final_results = {}\n",
    "for metric_name in COMPUTE_METRICS:\n",
    "    input_metric_name = \"input_\" + metric_name\n",
    "    ldf = all_metrics_df[metric_name] - all_metrics_df[input_metric_name]\n",
    "    final_results[metric_name] = all_metrics_df[metric_name].mean()\n",
    "    final_results[metric_name + \"_imp\"] = ldf.mean()\n",
    "\n",
    "print(\"Overall metrics :\")\n",
    "pprint(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
