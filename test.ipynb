{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DCCRN\n",
    "import torch\n",
    "from asteroid.data import DNSDataset\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import shutil\n",
    "import numpy as np\n",
    "from framework import ABF, MultiResolutionSTFTLoss, SPKDLoss, build_review_kd, ReviewKD, hcl\n",
    "from asteroid.models import DCCRNet\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import config as cfg\n",
    "from tools_for_model import near_avg_index, max_index, min_index, Bar, cal_pesq, cal_stoi\n",
    "from dataloader import create_dataloader\n",
    "import inspect\n",
    "import os\n",
    "import feature_extraction\n",
    "from asteroid.models import DCCRNet_mini\n",
    "  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = DCCRN.DCCRN(rnn_units=cfg.rnn_units_student, masking_mode=cfg.masking_mode, use_clstm=cfg.use_clstm,kernel_num=cfg.kernel_num_student)\n",
    "#model = DCCRN.DCCRN(rnn_units=cfg.rnn_units, masking_mode=cfg.masking_mode, use_clstm=cfg.use_clstm,kernel_num=cfg.kernel_num)\n",
    "#data_set =  DNSDataset(\"/root/NTH_student/Speech_Enhancement_new/asteroid/egs/dns_challenge/baseline/data\")\n",
    "#model = DCCRNet.from_pretrained('JorisCos/DCCRNet_Libri1Mix_enhsingle_16k')\n",
    "model = DCCRNet_mini(architecture='DCCRN-CL-test')\n",
    "train_dataset =  DNSDataset(\"/root/NTH_student/test_loader\")\n",
    "train_loader = create_dataloader(mode='train',dataset=train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tạo sample để input mode. cái này random đại thui\n",
    "# input_size = model.stft.weight.size(1)\n",
    "# tạo cái batch có n= 5 đi\n",
    "# input = torch.randn(64, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tạo list chứa các feature map do encoder output ra \n",
    "# # dùng list vì lát nữa feature map của lstm với decoder bỏ vô đây luôn. ở đây demo cái encoder thui nha. \n",
    "# feature_maps_encoder = []\n",
    "# feature_maps_decoder = []\n",
    "# # Tạo hàm hook, hàm này sẽ thực thi khi encoder thực hiện forward.\n",
    "# # output của encoder sẽ được append vào trong encoder_feature_map(một biến toàn cục)\n",
    "# def encoder_hook(module, input, output):\n",
    "#     feature_maps_encoder.append(output)\n",
    "# # đăng kí forward hook. kiểu \"ê khi nào encoder thực hiện foward xong thì chạy hàm encoder_hook nha\"\n",
    "\n",
    "# [model.encoder[i].register_forward_hook(encoder_hook) for i in range(len(model.encoder))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decoder_hook(module, input, output):\n",
    "#     feature_maps_decoder.append(output)\n",
    "\n",
    "# for i in range(0,len(model.decoder)):\n",
    "#     model.decoder[i].register_forward_hook(decoder_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def enhance_hook(module, input, output):\n",
    "#     feature_maps.append(output)\n",
    "\n",
    "# model.enhance.register_forward_hook(enhance_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 64/600: [==>...............................................] - ETA 75.5s"
     ]
    }
   ],
   "source": [
    "X,Y = 0,0\n",
    "i = 0\n",
    "for x,y,_ in Bar(train_loader):\n",
    "    if i == 1: break\n",
    "    X=x\n",
    "    Y=y\n",
    "    i += 1\n",
    "\n",
    "# try :\n",
    "#     model(X)\n",
    "#     print('au de')\n",
    "# except:\n",
    "#     print('failed roihuhu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps_extraction = feature_extraction.DCCRNet(model)\n",
    "feature_maps = feature_maps_extraction.extract_feature_maps(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_maps[\"clstm_real\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_maps[\"clstm_img\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 471, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_maps[\"clstm_real\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_maps['encoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 8, 128, 476])\n",
      "torch.Size([32, 16, 64, 475])\n",
      "torch.Size([32, 32, 32, 474])\n",
      "torch.Size([32, 64, 16, 473])\n",
      "torch.Size([32, 64, 8, 472])\n",
      "torch.Size([32, 64, 4, 471])\n"
     ]
    }
   ],
   "source": [
    "feature_maps_encod = feature_maps['encoder']\n",
    "for i in range(6):\n",
    "    print(feature_maps_encod[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 4, 471])\n",
      "torch.Size([32, 64, 8, 472])\n",
      "torch.Size([32, 64, 16, 473])\n",
      "torch.Size([32, 32, 32, 474])\n",
      "torch.Size([32, 16, 64, 475])\n",
      "torch.Size([32, 8, 128, 476])\n"
     ]
    }
   ],
   "source": [
    "feature_maps_encod = feature_maps['decoder']\n",
    "for i in range(6):\n",
    "    print(feature_maps_encod[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8, 128, 476])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_maps['encoder'][0].shape #[64, 64, 4, 483]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64, 16, 473])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_maps['decoder'][2].shape #[64, 2, 256, 484]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = [8, 16, 32, 64, 64, 64]\n",
    "out_channels = [32, 64, 128, 256, 256, 256]\n",
    "\n",
    "# shapes = [1,4,8,16,32]\n",
    "# out_shapes = [1,4,8,16,32]\n",
    "shapes = [4,8,16,32,64,128]\n",
    "out_shapes = [4,8,16,32,64,128]\n",
    "abfs = nn.ModuleList()\n",
    "mid_channel = min(512, in_channels[-1])\n",
    "for idx, in_channel in enumerate(in_channels):\n",
    "    abfs.append(ABF(in_channel, mid_channel, out_channels[idx], idx < len(in_channels)-1))\n",
    "abfs = abfs[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "x = feature_maps['encoder'][::-1] #encoder\n",
    "\n",
    "\n",
    "results = []\n",
    "out_features, res_features = abfs[0](x[0].cuda(), out_shape=out_shapes[0],feature_type=\"encoder\")\n",
    "results.append(out_features)\n",
    "for feature, abf, shape, out_shape in zip(x[1:], abfs[1:], shapes[1:], out_shapes[1:]):\n",
    "        out_features, res_features = abf(feature.cuda(), res_features, shape, out_shape,\"encoder\")\n",
    "        print(\"1\")\n",
    "        results.insert(0, out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32, 128, 476])\n",
      "torch.Size([32, 64, 64, 475])\n",
      "torch.Size([32, 128, 32, 474])\n",
      "torch.Size([32, 256, 16, 473])\n",
      "torch.Size([32, 256, 8, 472])\n",
      "torch.Size([32, 256, 4, 471])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(results)):\n",
    "    print(results[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# re = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "# im = torch.tensor([[5.0, 6.0], [7.0, 8.0]])\n",
    "\n",
    "# complex_tensor = torch.view_as_complex(torch.stack([re, im], dim=-1))\n",
    "# h = torch.stack([re, im])\n",
    "# h.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = [8, 16, 32, 64, 64, 64]\n",
    "out_channels = [32, 64, 128, 256, 256, 256]\n",
    "\n",
    "# shapes = [1,4,8,16,32]\n",
    "# out_shapes = [1,4,8,16,32]\n",
    "shapes = [4,8,16,32,64,128]\n",
    "out_shapes = [4,8,16,32,64,128]\n",
    "abfs = nn.ModuleList()\n",
    "mid_channel = min(512, in_channels[-1])\n",
    "for idx, in_channel in enumerate(in_channels):\n",
    "    abfs.append(ABF(in_channel, mid_channel, out_channels[idx], idx < len(in_channels)-1))\n",
    "abfs = abfs[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "x = feature_maps['decoder']\n",
    "results = []\n",
    "out_features, res_features = abfs[0](x[0].cuda(), out_shape=out_shapes[0],feature_type=\"decoder\")\n",
    "results.append(out_features)\n",
    "for feature, abf, shape, out_shape in zip(x[1:], abfs[1:], shapes[1:], out_shapes[1:]):\n",
    "        out_features, res_features = abf(feature.cuda(), res_features, shape, out_shape,feature_type=\"decoder\")\n",
    "        print(\"1\")\n",
    "        results.append(out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 256, 4, 471])\n",
      "torch.Size([32, 256, 8, 472])\n",
      "torch.Size([32, 256, 16, 473])\n",
      "torch.Size([32, 128, 32, 474])\n",
      "torch.Size([32, 64, 64, 475])\n",
      "torch.Size([32, 32, 128, 476])\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(results[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = hcl(results[1],results[1],'encoder')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 471, 128])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stu_img = feature_maps['clstm_img'][0]\n",
    "stu_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input dimension should be at least 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m l  \u001b[39m=\u001b[39m hcl(feature_maps[\u001b[39m'\u001b[39;49m\u001b[39mclstm_img\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m],feature_maps[\u001b[39m'\u001b[39;49m\u001b[39mclstm_img\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m],\u001b[39m'\u001b[39;49m\u001b[39mlstm\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/NTH_student/Speech_Enhancement_new/knowledge_distillation_CLSKD/framework.py:299\u001b[0m, in \u001b[0;36mhcl\u001b[0;34m(fstudent, fteacher, t_type)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39mif\u001b[39;00m l \u001b[39m>\u001b[39m\u001b[39m=\u001b[39mh:\n\u001b[1;32m    298\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 299\u001b[0m tmpfs \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49madaptive_avg_pool2d(fs, (l,l))\n\u001b[1;32m    300\u001b[0m tmpft \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39madaptive_avg_pool2d(ft, (l,l))\n\u001b[1;32m    301\u001b[0m cnt \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2.0\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1213\u001b[0m, in \u001b[0;36madaptive_avg_pool2d\u001b[0;34m(input, output_size)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39minput\u001b[39m):\n\u001b[1;32m   1212\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(adaptive_avg_pool2d, (\u001b[39minput\u001b[39m,), \u001b[39minput\u001b[39m, output_size)\n\u001b[0;32m-> 1213\u001b[0m _output_size \u001b[39m=\u001b[39m _list_with_default(output_size, \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msize())\n\u001b[1;32m   1214\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39madaptive_avg_pool2d(\u001b[39minput\u001b[39m, _output_size)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/utils.py:35\u001b[0m, in \u001b[0;36m_list_with_default\u001b[0;34m(out_size, defaults)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[39mreturn\u001b[39;00m out_size\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(defaults) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(out_size):\n\u001b[0;32m---> 35\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     36\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInput dimension should be at least \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(out_size) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m     37\u001b[0m     )\n\u001b[1;32m     38\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m     39\u001b[0m     v \u001b[39mif\u001b[39;00m v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m d \u001b[39mfor\u001b[39;00m v, d \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(out_size, defaults[\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(out_size) :])\n\u001b[1;32m     40\u001b[0m ]\n",
      "\u001b[0;31mValueError\u001b[0m: Input dimension should be at least 3"
     ]
    }
   ],
   "source": [
    "l  = hcl(feature_maps['clstm_img'][0],feature_maps['clstm_img'][0],'lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stu_rea = feature_maps['clstm_real'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DCCRNet.from_pretrained('JorisCos/DCCRNet_Libri1Mix_enhsingle_16k')\n",
    "feature_maps_extraction = feature_extraction.DCCRNet(model)\n",
    "feature_maps = feature_maps_extraction.extract_feature_maps(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tea_img = feature_maps['clstm_img'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tea_rea = feature_maps['clstm_real'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0015, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kd_loss = SPKDLoss(stu_rea, tea_rea,reduction='batchmean')\n",
    "loss = kd_loss()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0011, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kd_loss = SPKDLoss(stu_img, tea_img,reduction='batchmean')\n",
    "loss = kd_loss()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_maps.clear()\n",
    "feature_maps_extraction.remove_hook()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
